# Diffusion Model Implementation Summary
## A Complete Beginner's Guide to Understanding Image Generation with AI

## üéØ What Is This Project?

Imagine you have a photograph, and you gradually add more and more random noise to it until it becomes completely unrecognizable‚Äîjust random static. Now imagine teaching a computer program to **reverse this process**: starting from pure noise and gradually removing it until a clear image appears.

That's exactly what a **Diffusion Model** does! This project is a complete, working implementation of such a system, built from scratch to help you understand how modern AI image generators (like Stable Diffusion, DALL-E, and Midjourney) actually work under the hood.

### What You'll Learn
By working through this project, you'll understand:
- How images can be systematically destroyed and recreated
- How neural networks learn to reverse this destruction
- Why this approach generates high-quality, diverse images
- The mathematics and code that power modern AI art generators

## üì¶ Understanding the Architecture: A Deep Dive

### Part 1: The Foundation - What is a Neural Network?

Before diving into diffusion models, let's understand the basics:

**Neural Networks** are computer programs inspired by how our brains work. They consist of:
- **Layers**: Think of these as processing stages, like an assembly line
- **Neurons**: Individual units that perform simple calculations
- **Weights**: Numbers that get adjusted during "learning" to make better predictions
- **Training**: The process of showing examples to the network so it learns patterns

In our project, we use a special type of neural network called a **U-Net**.

---

### Part 2: The Core Components (Files Explained)

#### 1. **diffusion.py** - The Mathematical Engine

This file contains the `DiffusionProcess` class (lines 11-211), which implements the core mathematics of how we add and remove noise from images.

**Key Concept: What is "Diffusion"?**
- In physics, diffusion is when particles spread out (like a drop of ink in water)
- In our model, we "diffuse" an image by gradually adding random noise until it's unrecognizable
- Then we train a model to reverse this process

**Two Main Processes:**

**A. Forward Diffusion (Adding Noise)** - Lines 93-120
```python
def forward_diffusion(self, x_0, t, noise=None):
```
- **What it does**: Takes a clean image (`x_0`) and adds noise to it
- **How much noise?**: Depends on the timestep `t` (0 = clean, 999 = pure noise)
- **The formula** (line 96-97):
  ```
  x_t = sqrt(Œ±_t) * x_0 + sqrt(1-Œ±_t) * Œµ
  ```
  - `x_0`: Your original clean image
  - `x_t`: The noisy image at step t
  - `Œ±_t`: A number that controls how much of the original image remains
  - `Œµ`: Random noise (like TV static)
  
- **Why this formula?**: It's a mathematical trick that lets us add noise in a controlled, predictable way
- **No training needed**: This is just pure mathematics!

**B. Reverse Diffusion (Removing Noise)** - Lines 146-177
```python
def reverse_diffusion_step(self, model, x_t, t, t_index):
```
- **What it does**: Takes a noisy image and makes it slightly cleaner
- **How?**: Uses our trained neural network to predict what noise was added
- **The process**: Start with pure noise, remove a little bit 1000 times ‚Üí get a clean image!
- **This requires training**: The model must learn how to denoise

**C. Noise Schedules** - Lines 37-85
```python
def _linear_schedule(self, beta_start, beta_end, num_timesteps):
def _cosine_schedule(self, num_timesteps, s=0.008):
```
- **What are these?**: Plans for how aggressively to add noise at each step
- **Linear schedule**: Adds noise at a constant rate (simple, works well)
- **Cosine schedule**: Adds noise more gradually at first, then faster (more stable)
- **Think of it like**: Different strategies for gradually fading out a photograph

---

#### 2. **models/unet.py** - The Neural Network Brain

This file defines the U-Net architecture - the "brain" that learns to remove noise.

**Key Concept: What is a U-Net?**
- A U-Net is a neural network shaped like the letter "U"
- It has two paths: **down** (compress information) and **up** (expand back)
- **Skip connections**: Shortcuts that help preserve details
- Originally designed for medical image analysis, now used in many image tasks

**A. TimeEmbedding Class** - Lines 11-34
```python
class TimeEmbedding(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
    
    def forward(self, t):
        # Creates sinusoidal embeddings
```
- **Purpose**: Converts a timestep number (like 500) into a rich vector of numbers
- **Why?**: The model needs to know "how noisy is this image?" to denoise correctly
- **How it works**: Uses sine and cosine waves at different frequencies (similar to how our ears process sound frequencies)
- **Think of it like**: A barcode that encodes the timestep information

**B. DownBlock Class** - Lines 37-77
```python
class DownBlock(nn.Module):
    def __init__(self, in_channels, out_channels, time_emb_dim):
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.time_mlp = nn.Linear(time_emb_dim, out_channels)
        self.norm1 = nn.GroupNorm(8, out_channels)
        self.norm2 = nn.GroupNorm(8, out_channels)
```
- **Purpose**: A building block that processes and compresses image features
- **Components**:
  - `Conv2d`: **Convolutional layers** - scan the image with small filters to detect patterns (like edges, textures)
  - `GroupNorm`: **Normalization** - keeps numbers in a reasonable range (like volume control)
  - `SiLU`: **Activation function** - adds non-linearity (lets the network learn complex patterns)
  - `time_mlp`: Injects timestep information into the features
- **Think of it like**: A pair of glasses that focuses on important features while remembering what time it is

**C. UpBlock Class** - Lines 80-128
```python
class UpBlock(nn.Module):
    def forward(self, x, skip, t):
        # Concatenate with skip connection
        x = torch.cat([x, skip], dim=1)
```
- **Purpose**: A building block that expands features back to image size
- **Special feature**: `skip` - receives information from the corresponding DownBlock
- **Why skip connections?**: Helps recover fine details that were lost during compression
- **Think of it like**: Reconstructing a photo while referring to notes you made earlier

**D. SimpleUNet Class** - Lines 131-252
```python
class SimpleUNet(nn.Module):
    def __init__(self, image_channels=1, base_channels=64, time_emb_dim=256):
        # Encoder (downsampling path)
        self.down1 = DownBlock(base_channels, base_channels, time_emb_dim)
        self.down2 = DownBlock(base_channels, base_channels * 2, time_emb_dim)
        
        # Bottleneck
        self.bottleneck = DownBlock(base_channels * 2, base_channels * 2, time_emb_dim)
        
        # Decoder (upsampling path)
        self.up1 = UpBlock(base_channels * 4, base_channels * 2, time_emb_dim)
        self.up2 = UpBlock(base_channels * 3, base_channels, time_emb_dim)
```
- **The Complete Architecture**:
  ```
  Input (28x28 noisy image + timestep)
       ‚Üì
  Initial Conv ‚Üí Extract basic features
       ‚Üì
  DownBlock 1 (28x28 ‚Üí 14x14) ‚îÄ‚îÄ‚îê (save for skip)
       ‚Üì                         ‚îÇ
  DownBlock 2 (14x14 ‚Üí 7x7)  ‚îÄ‚îÄ‚îÄ‚î§ (save for skip)
       ‚Üì                         ‚îÇ
  Bottleneck (7x7)               ‚îÇ
       ‚Üì                         ‚îÇ
  UpBlock 1 (7x7 ‚Üí 14x14) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò (receive skip)
       ‚Üì                         ‚îÇ
  UpBlock 2 (14x14 ‚Üí 28x28) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îò (receive skip)
       ‚Üì
  Final Conv ‚Üí Predicted noise (28x28)
  ```
  
- **Parameters**: ~2 million trainable weights (the numbers that get adjusted during training)
- **Input**: A noisy image (28√ó28 pixels) + a timestep number
- **Output**: The predicted noise (same size as input)
- **Think of it like**: An hourglass that squeezes information down, then expands it back up, with shortcuts to preserve details

---

#### 3. **train.py** - The Training Pipeline

This is where the magic happens - where the model learns to denoise!

**Key Concept: What is Training?**
- **Training** is the process of showing the model many examples and letting it adjust its internal parameters (weights) to get better at its task
- Like teaching a child by showing them examples: "This is a cat", "This is a dog", repeat 1000 times
- The model makes predictions, we tell it how wrong it was, and it adjusts to do better next time

**A. The Training Loop** - Lines 19-60 (`train_epoch` function)
```python
def train_epoch(model, dataloader, optimizer, diffusion, device):
    for batch_idx, (images, _) in enumerate(tqdm(dataloader, desc="Training")):
        # 1. Get clean images
        images = images.to(device)
        
        # 2. Pick random timesteps for each image
        t = get_timesteps(batch_size, diffusion.num_timesteps, device)
        
        # 3. Add noise (forward diffusion)
        noisy_images, noise = diffusion.forward_diffusion(images, t)
        
        # 4. Ask model to predict the noise
        predicted_noise = model(noisy_images, t)
        
        # 5. Calculate how wrong the prediction was (loss)
        loss = nn.functional.mse_loss(predicted_noise, noise)
        
        # 6. Update model weights to reduce error
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

**Step-by-step breakdown:**
1. **Load images**: Get a batch of clean digit images (128 images at once)
2. **Random timesteps**: For each image, pick a random noise level (t could be 50, 300, 800, etc.)
3. **Add noise**: Apply forward diffusion to create noisy versions
4. **Predict**: Ask the model "What noise was added?"
5. **Calculate error**: Compare prediction to actual noise using **MSE (Mean Squared Error)**
   - MSE = Average of (predicted - actual)¬≤
   - Lower is better!
6. **Update weights**: Use **backpropagation** (calculus magic) to adjust the model's internal numbers

**Key Concept: What is Loss?**
- **Loss** is a number that measures how wrong the model's predictions are
- Think of it like: "How far off target is the arrow?"
- During training, loss should go down (model getting better)
- We use **MSE Loss**: `loss = average((prediction - truth)¬≤)`

**Key Concept: What is an Optimizer?**
- An **optimizer** is an algorithm that adjusts the model's weights to reduce loss
- We use **Adam optimizer** (line 140) - a popular, effective choice
- Learning rate (default: 0.0002) controls how big each adjustment is
- Think of it like: The optimizer is a coach telling the model how to improve

**B. Generating Samples During Training** - Lines 63-78
```python
def sample_images(model, diffusion, device, num_images=64):
    model.eval()
    samples = diffusion.sample(model, image_size=28, batch_size=num_images, channels=1)
    return samples
```
- **Purpose**: Periodically generate images to see if training is working
- **When**: Every epoch (or every N epochs)
- **Why**: Visual feedback is more intuitive than just looking at loss numbers
- **Think of it like**: Taking progress photos while learning to paint

**C. Main Training Function** - Lines 81-218
```python
def train(
    dataset_name="mnist",
    batch_size=128,
    num_epochs=20,
    learning_rate=2e-4,
    ...
):
```
- **Hyperparameters** (settings you can adjust):
  - `batch_size=128`: Process 128 images at once (larger = faster but more memory)
  - `num_epochs=20`: Go through the entire dataset 20 times
  - `learning_rate=2e-4`: How aggressively to update weights (0.0002)
  - `num_timesteps=1000`: How many diffusion steps

**Key Concept: What is an Epoch?**
- One **epoch** = going through the entire training dataset once
- If you have 60,000 images and batch_size=128, one epoch = 468 batches
- More epochs = more training = (usually) better results

**Key Concept: What is Batch Size?**
- **Batch size** = number of images processed together before updating weights
- Larger batches:
  - Faster training (better hardware utilization)
  - More memory needed
  - Smoother gradient updates
- Smaller batches:
  - Slower training
  - Less memory
  - Noisier but sometimes helps generalization

---

#### 4. **sample.py** - Generating New Images

After training, this script generates new images from pure noise!

**The Generation Process**:
```python
# Start with pure random noise
x = torch.randn(64, 1, 28, 28)  # 64 noisy images

# Gradually denoise 1000 times
for t in reversed(range(1000)):
    x = model.denoise_one_step(x, t)

# Final result: 64 generated digit images!
```

**Key Points**:
- Takes about 30 seconds on CPU, 5 seconds on GPU
- Each image goes through 1000 denoising steps
- Can save intermediate steps to see the process (`--show_intermediate`)
- The more you trained, the better the results!

---

#### 5. **visualize_diffusion.py** - Educational Visualization

This script helps you understand forward diffusion by showing it visually.

**What it shows** (lines 60-85):
```python
for i, img in enumerate(images):
    for j, t in enumerate(timesteps_to_show):
        # Add noise at timestep t
        noisy_img, _ = diffusion.forward_diffusion(img.unsqueeze(0), t_tensor)
        # Plot it
```
- Takes clean images
- Shows them at different noise levels: t=0, 50, 100, 200, 400, 600, 800, 999
- Creates a grid: each row is one image, each column is a different noise level
- **Output**: You literally see images gradually becoming noise!

**Why this is helpful**:
- Builds intuition for what "forward diffusion" means
- Shows why the model needs to know the timestep (different noise levels look very different)
- Demonstrates that at t=999, images are just pure noise (no information left)

---

#### 6. **utils.py** - Helper Functions

Contains utility functions used throughout the project:

**A. `save_images` function** - Lines 11-32
- Saves a batch of images as a grid
- Handles normalization from [-1,1] to [0,1]
- Creates nice-looking grids with padding

**B. `plot_losses` function** - Lines 35-61
- Plots training loss over time
- Adds a moving average for smoother visualization
- Saves as PNG file

**C. `count_parameters` function** - Lines 64-66
- Counts how many trainable parameters (weights) the model has
- Our U-Net has ~2 million parameters

**D. `get_device` function** - Lines 69-74
- Automatically detects if you have a GPU (CUDA) or Apple Silicon (MPS)
- Falls back to CPU if no GPU available
- GPU makes training 10-50x faster!

---

## üîë Key Concepts Explained (The "Why" Behind Everything)

### 1. Why Predict Noise Instead of Images?

You might wonder: "Why does the model predict *noise* rather than the clean image directly?"

**Answer**: It's easier and more stable!

- **Direct prediction**: "Given this noisy mess, what's the clean image?" ‚Üí Very hard!
- **Noise prediction**: "Given this noisy image, what noise was added?" ‚Üí Much easier!

**Analogy**: 
- Direct: "This photo has coffee stains. Reconstruct the original." (Hard!)
- Noise: "This photo has coffee stains. Draw the coffee stains." (Easier!)

Once you know the noise, you can subtract it to get the clean image:
```
clean_image = noisy_image - predicted_noise
```

### 2. Why 1000 Timesteps?

Why not just add all the noise at once and remove it all at once?

**Answer**: Small steps are easier to learn!

- **One big step**: "Turn this noise into a perfect image" ‚Üí Too complex
- **1000 tiny steps**: "Make this slightly less noisy" ‚Üí Much simpler

**Analogy**:
- Big step: "Jump from ground to roof" (impossible)
- Small steps: "Climb stairs one at a time" (easy)

Each step the model only needs to remove a tiny bit of noise, which is a much easier task to learn.

### 3. Why Use Time Embeddings?

The model needs to know "how noisy is this image?" to denoise correctly.

**The Problem**:
- At t=50: Image is barely noisy, need gentle denoising
- At t=900: Image is very noisy, need aggressive denoising
- **Same model, different behavior needed!**

**The Solution**: Time embeddings!
- Convert timestep number into a vector
- Feed this vector into the model
- Model learns to adjust its behavior based on the timestep

**Analogy**: Like telling a cleaning crew "light cleaning" vs "deep cleaning"

### 4. Understanding the Mathematics

**The Forward Diffusion Formula** (diffusion.py, line 96):
```
x_t = sqrt(Œ±_t) * x_0 + sqrt(1-Œ±_t) * Œµ
```

**What this means in plain English**:
- `x_0`: Original clean image (100% signal, 0% noise)
- `x_t`: Image at timestep t (mix of signal and noise)
- `Œ±_t`: "How much original image to keep" (starts at ~1.0, ends at ~0.0)
- `Œµ`: Random noise
- As t increases: Œ±_t decreases, so more noise, less original image

**Example**:
- t=0: `x_0 = 1.0 * original + 0.0 * noise` ‚Üí Pure original
- t=500: `x_500 = 0.44 * original + 0.90 * noise` ‚Üí Half destroyed
- t=999: `x_999 = 0.006 * original + 1.0 * noise` ‚Üí Pure noise

**Why this specific formula?**
- It has nice mathematical properties (Gaussian distributions stay Gaussian)
- We can jump to any timestep without computing all previous steps
- It's theoretically grounded in probability theory

### 5. The Training Objective

**What we're optimizing** (train.py, line 48):
```python
loss = MSE(predicted_noise, actual_noise)
```

**In mathematical notation**:
```
L = E[ ||Œµ - Œµ_Œ∏(x_t, t)||¬≤ ]
```

**Translation**:
- `Œµ`: The actual noise we added
- `Œµ_Œ∏(x_t, t)`: What the model predicts (Œ∏ represents model parameters)
- `|| ... ||¬≤`: Squared difference (MSE)
- `E[...]`: Average over all training examples and timesteps

**Goal**: Make predicted noise as close as possible to actual noise

**Why MSE?**
- Simple and effective
- Penalizes large errors more than small ones
- Smooth gradient for optimization
- Corresponds to maximizing likelihood under Gaussian assumptions

### 6. Skip Connections in U-Net

**The Problem Without Skip Connections**:
```
28x28 ‚Üí 14x14 ‚Üí 7x7 ‚Üí 14x14 ‚Üí 28x28
  ‚Üì       ‚Üì       ‚Üì       ‚Üì       ‚Üì
Details lost during compression can't be recovered!
```

**The Solution With Skip Connections**:
```
28x28 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚Üì                         ‚Üì
14x14 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚Üì
  ‚Üì            ‚Üì            ‚Üì
7x7            ‚Üì            ‚Üì
  ‚Üì            ‚Üì            ‚Üì
14x14 ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚Üì
  ‚Üì                         ‚Üì
28x28 ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Result**: Decoder has access to both high-level features AND low-level details!

**Analogy**: 
- Without skips: Like describing a painting from memory after looking away
- With skips: Like describing a painting while still looking at it

### 7. Why Train on Random Timesteps?

In `train.py` (line 42), we randomly sample timesteps:
```python
t = get_timesteps(batch_size, diffusion.num_timesteps, device)
```

**Why random?**
- The model needs to handle ALL noise levels (t=0 to t=999)
- Random sampling ensures balanced training across all timesteps
- Prevents overfitting to specific noise levels

**What happens**:
- Batch 1: t = [234, 567, 89, 901, ...] (random)
- Batch 2: t = [12, 788, 345, 522, ...] (random)
- Over time: Model sees all possible timesteps equally

**Analogy**: Teaching someone to recognize faces at all distances, not just up close

### 8. Normalization: Why [-1, 1]?

Images are normalized to the range [-1, 1] (train.py, line 117):
```python
transforms.Normalize((0.5,), (0.5,))
```

**Why this range?**
- Zero-centered: Easier for neural networks to learn
- Symmetric: Noise can be both positive and negative
- Matches Gaussian noise: Noise has mean 0
- Standard practice in deep learning

**Conversion**:
- Original: [0, 255] (pixel values)
- After ToTensor: [0, 1] (normalized)
- After Normalize: [-1, 1] (zero-centered)

---

## ÔøΩ How Everything Works Together: The Complete Picture

Let me walk you through what happens when you run the full pipeline:

### Stage 1: Visualization (Understanding the Problem)

```bash
python visualize_diffusion.py
```

**What happens**:
1. Loads MNIST dataset (60,000 handwritten digit images)
2. Picks 8 random images
3. For each image, shows it at timesteps: 0, 50, 100, 200, 400, 600, 800, 999
4. Saves visualization to `outputs/forward_diffusion.png`

**What you see**:
- Left column (t=0): Clean, recognizable digits
- Middle columns: Gradual degradation
- Right column (t=999): Pure noise, no digits visible

**What you learn**: This is the process we need to reverse!

---

### Stage 2: Training (Learning to Reverse)

```bash
python train.py --epochs 20
```

**What happens (simplified)**:
```
For 20 epochs:
    For each batch of 128 images:
        1. Get clean images (e.g., digit "7")
        2. Pick random timesteps (e.g., t=342)
        3. Add noise to create x_342
        4. Ask model: "What noise was added?"
        5. Model predicts noise
        6. Compare prediction to actual noise ‚Üí Loss
        7. Update model weights to reduce loss
    
    Generate 64 sample images to track progress
    Save checkpoint
    
Final: Save trained model to outputs/model_final.pt
```

**What the model learns**:
- Early epochs: Random guessing, loss ~0.10, samples look like noise
- Middle epochs: Starting to denoise, loss ~0.04, samples show blurry digit shapes
- Late epochs: Good denoising, loss ~0.02, samples show clear digits

**Files created**:
- `outputs/samples/epoch_001.png` through `epoch_020.png`: Progress visualization
- `outputs/checkpoints/model_epoch_005.pt`, etc.: Saved model weights
- `outputs/training_loss.png`: Loss curve showing improvement
- `outputs/model_final.pt`: Final trained model

---

### Stage 3: Generation (Creating New Images)

```bash
python sample.py --checkpoint outputs/model_final.pt --num_images 64
```

**What happens (step-by-step)**:
```
1. Load trained model from checkpoint
2. Create 64 images of pure random noise: x_999
3. For t from 999 down to 0:
       x_t-1 = denoise(x_t, t, model)
4. After 1000 steps: x_0 = clean generated images!
5. Save to outputs/generated_samples.png
```

**Detailed denoising process**:
```
Step 999: Pure noise ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (nothing recognizable)
Step 800: Still very noisy ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì
Step 600: Vague shapes emerging ‚ñì‚ñì‚ñë‚ñë‚ñì‚ñì‚ñë‚ñë
Step 400: Digit structure visible ‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñë
Step 200: Clearer digit ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
Step 0: Clean digit   3  (generated!)
```

**What you get**:
- 64 brand new digit images that never existed before
- Each one generated from random noise
- Quality depends on how well you trained
- With `--show_intermediate`: See the full denoising process frame by frame

---

## üéì Complete Learning Path

### Phase 1: Understanding (No Coding Required)

**Concepts to grasp**:
- [ ] What is a neural network? (Layers, weights, training)
- [ ] What is forward diffusion? (Adding noise gradually)
- [ ] What is reverse diffusion? (Removing noise gradually)
- [ ] Why predict noise instead of images? (Easier to learn)
- [ ] What is a U-Net? (Encoder-decoder with skip connections)
- [ ] What are time embeddings? (How model knows noise level)

**Activities**:
1. Read this SUMMARY.md thoroughly
2. Run `python visualize_diffusion.py` and study the output
3. Watch the forward diffusion happen
4. Read the code comments in `diffusion.py`

---

### Phase 2: Training Your First Model (Hands-on)

**Goal**: Train a model and see it learn

**Steps**:
```bash
# 1. Quick training (10 minutes)
python train.py --epochs 10

# Watch the outputs/ folder:
# - samples/ will show improving quality
# - training_loss.png will show decreasing loss

# 2. Generate images
python sample.py --checkpoint outputs/model_final.pt

# 3. Generate with intermediate steps
python sample.py --checkpoint outputs/model_final.pt --show_intermediate
```

**What to observe**:
- Training loss should decrease (from ~0.10 to ~0.03)
- Sample quality improves each epoch
- Generated images look like real digits (but are new!)

**Experiments**:
- Try different batch sizes: `--batch_size 64` vs `--batch_size 256`
- Try Fashion-MNIST: `--dataset fashion_mnist`
- Train longer: `--epochs 50` (better quality)

---

### Phase 3: Understanding the Code (Deep Dive)

**Read in this order**:

1. **utils.py** (simplest, helpers)
   - Understand save_images, plot_losses
   - These are just utilities, nothing complex

2. **diffusion.py** (core math)
   - Start with `forward_diffusion` function
   - Understand the formula: x_t = sqrt(Œ±_t) * x_0 + sqrt(1-Œ±_t) * Œµ
   - Look at noise schedules: linear vs cosine
   - Study `reverse_diffusion_step`

3. **models/unet.py** (neural network)
   - Start with TimeEmbedding: how timesteps are encoded
   - Understand DownBlock: what happens during downsampling
   - Understand UpBlock: what happens during upsampling
   - See how skip connections work
   - Study the full SimpleUNet architecture

4. **train.py** (bringing it together)
   - Understand the training loop
   - See how loss is calculated
   - Understand how samples are generated
   - Look at optimizer and learning rate

5. **sample.py** (generation)
   - See how sampling works
   - Understand the reverse loop (t=999 to t=0)
   - Optional: study intermediate visualization

---

### Phase 4: Experimentation (Make It Your Own)

**Experiment 1: Training Duration**
```bash
python train.py --epochs 10 --output_dir outputs/exp1_10ep
python train.py --epochs 30 --output_dir outputs/exp1_30ep
python train.py --epochs 100 --output_dir outputs/exp1_100ep
```
**Question**: How does training time affect quality? Is there a point of diminishing returns?

**Experiment 2: Batch Size**
```bash
python train.py --batch_size 32
python train.py --batch_size 128
python train.py --batch_size 512  # if you have enough memory
```
**Question**: How does batch size affect training speed and final quality?

**Experiment 3: Learning Rate**
```bash
python train.py --lr 1e-4  # slower learning
python train.py --lr 2e-4  # default
python train.py --lr 5e-4  # faster learning
```
**Question**: What happens if learning rate is too high or too low?

**Experiment 4: Datasets**
```bash
python train.py --dataset mnist --epochs 20
python train.py --dataset fashion_mnist --epochs 20
```
**Question**: Which dataset is harder? Why? Compare the loss curves.

**Experiment 5: Timesteps**
Edit train.py to try different timestep counts:
- 500 steps: Faster but possibly lower quality
- 1000 steps: Default, good balance
- 2000 steps: Slower but potentially better

**Question**: Is more always better? What's the trade-off?

---

### Phase 5: Extensions (Advanced)

Once you're comfortable with the basics, try these extensions:

**Extension 1: Class-Conditional Generation**
- Modify model to take class labels as input
- Generate specific digits on demand: "Generate a 7"
- Hint: Add class embedding similar to time embedding

**Extension 2: Faster Sampling (DDIM)**
- Implement DDIM algorithm
- Sample in 50 steps instead of 1000
- 20x faster generation!

**Extension 3: Different Image Sizes**
- Modify U-Net for 32√ó32 or 64√ó64 images
- Train on CIFAR-10 (color images)
- Requires more parameters and training time

**Extension 4: Better Visualizations**
- Plot attention maps to see what model focuses on
- Visualize embeddings using t-SNE
- Create GIFs of the generation process

**Extension 5: Architecture Experiments**
- Try different U-Net depths
- Experiment with different activation functions
- Add attention mechanisms

---

## üìä What to Expect: Results and Performance

### Training Time Estimates

**On CPU (typical laptop)**:
- 10 epochs: ~10 minutes
- 20 epochs: ~20 minutes
- 50 epochs: ~50 minutes
- 100 epochs: ~2 hours

**On GPU (NVIDIA, 8GB VRAM)**:
- 10 epochs: ~2 minutes
- 20 epochs: ~4 minutes
- 50 epochs: ~10 minutes
- 100 epochs: ~20 minutes

**Generation Time**:
- 64 images on CPU: ~30 seconds
- 64 images on GPU: ~5 seconds

---

### Quality Progression

**After 10 Epochs** (Quick test):
- **Loss**: ~0.04-0.06
- **Quality**: Blurry but recognizable digit shapes
- **Sample quality**: 60% look like digits, 40% unclear
- **Good for**: Understanding if everything works

**After 20 Epochs** (Default):
- **Loss**: ~0.025-0.035
- **Quality**: Clear digits with minor artifacts
- **Sample quality**: 80-90% look like good digits
- **Good for**: Learning and experimentation

**After 50 Epochs** (Recommended):
- **Loss**: ~0.020-0.028
- **Quality**: High-quality, sharp digits
- **Sample quality**: 90-95% excellent digits
- **Good for**: High-quality results, comparisons

**After 100 Epochs** (Thorough):
- **Loss**: ~0.015-0.025
- **Quality**: Very high quality, diverse
- **Sample quality**: 95%+ excellent, creative variations
- **Good for**: Best possible results

---

### Visual Quality Indicators

**Good signs** your model is learning:
- ‚úÖ Loss decreasing steadily
- ‚úÖ Sample digits become clearer each epoch
- ‚úÖ Diverse digit styles (different 3's, 7's, etc.)
- ‚úÖ Clean backgrounds, no artifacts
- ‚úÖ Proper digit structure (closed loops, separate strokes)

**Warning signs** something is wrong:
- ‚ùå Loss stuck or increasing
- ‚ùå All samples look the same (mode collapse)
- ‚ùå Checkerboard patterns (upsampling artifacts)
- ‚ùå NaN loss values (exploding gradients)
- ‚ùå Noise never fully removed from samples

---

### Expected Loss Curve

```
Loss
0.10 |*
     | *
0.08 |  *
     |   *
0.06 |    *
     |     **
0.04 |       ***
     |          ****
0.02 |              *******
     |_____________________
     0   5   10  15  20  Epochs
```

**Characteristics**:
- Rapid drop in first 5 epochs
- Gradual improvement 5-20 epochs
- Slow refinement after 20 epochs
- Eventually plateaus (model capacity limit)

---

## üî¨ Common Issues and Solutions

### Issue 1: Out of Memory (OOM)

**Symptoms**:
```
RuntimeError: CUDA out of memory
```

**Solutions**:
```bash
# Reduce batch size
python train.py --batch_size 64  # or 32, or 16

# Reduce model size (edit train.py, line ~134)
model = create_model(base_channels=32)  # default is 64

# Use gradient checkpointing (advanced)
```

---

### Issue 2: Training is Too Slow

**Solutions**:
```bash
# Check if GPU is being used
# Should print "Using device: cuda"
python train.py

# Reduce timesteps
python train.py --timesteps 500

# Increase batch size (if memory allows)
python train.py --batch_size 256

# Use fewer epochs for testing
python train.py --epochs 5
```

---

### Issue 3: Poor Quality Images

**Possible causes and fixes**:

**Not trained enough**:
```bash
# Train longer
python train.py --epochs 50
```

**Learning rate too high**:
```bash
# Reduce learning rate
python train.py --lr 1e-4
```

**Batch size too small**:
```bash
# Increase batch size for more stable gradients
python train.py --batch_size 128
```

**Model too small**:
```python
# In train.py, increase base_channels
model = create_model(base_channels=128)
```

---

### Issue 4: Loss is NaN

**Causes**: Gradient explosion, numerical instability

**Solutions**:
```bash
# Lower learning rate significantly
python train.py --lr 1e-5

# Add gradient clipping (edit train.py after line 53):
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
```

---

### Issue 5: Generated Images All Look Similar

**Cause**: Mode collapse (model learned limited diversity)

**Solutions**:
- Train longer
- Use different random seeds
- Try cosine noise schedule
- Increase model capacity
- Check if you're accidentally reusing the same noise

---

## üéØ Hardware Recommendations

### Minimum Requirements
- **CPU**: Any modern processor
- **RAM**: 4GB (8GB recommended)
- **Storage**: 2GB for data and outputs
- **Time**: Patience for CPU training

### Recommended Setup
- **CPU**: Multi-core (4+ cores)
- **RAM**: 16GB
- **GPU**: NVIDIA with 6GB+ VRAM (GTX 1060 or better)
- **Storage**: SSD for faster data loading

### Optimal Setup
- **CPU**: Any (GPU does the work)
- **RAM**: 16GB+
- **GPU**: NVIDIA RTX 3060 or better (8GB+ VRAM)
- **Storage**: NVMe SSD

### For Your Setup (16GB RAM, 8GB VRAM)
Your hardware is in the "Recommended" category! You can:
- ‚úÖ Train comfortably with batch_size=128 or 256
- ‚úÖ Use the default model size (base_channels=64)
- ‚úÖ Train for 50-100 epochs without issues
- ‚úÖ Generate large batches (100+ images at once)

**Recommended settings for your hardware**:
```bash
python train.py --batch_size 128 --epochs 50
python sample.py --num_images 100 --checkpoint outputs/model_final.pt
```

---

## üîç Code Architecture: Complete Flow Diagrams

### Training Loop (What Happens Each Batch)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Load Batch of Clean Images (128 images)         ‚îÇ
‚îÇ    images = [img1, img2, ..., img128]               ‚îÇ
‚îÇ    Shape: [128, 1, 28, 28]                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Sample Random Timesteps                          ‚îÇ
‚îÇ    t = [234, 567, 89, 901, ...]  (random for each) ‚îÇ
‚îÇ    Shape: [128]                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Forward Diffusion (Add Noise)                    ‚îÇ
‚îÇ    noise = random_noise()                           ‚îÇ
‚îÇ    noisy_images = sqrt(Œ±_t)*images + sqrt(1-Œ±_t)*noise ‚îÇ
‚îÇ    Code: diffusion.forward_diffusion()              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Model Prediction                                 ‚îÇ
‚îÇ    predicted_noise = model(noisy_images, t)        ‚îÇ
‚îÇ    Code: model(x, t) in unet.py                     ‚îÇ
‚îÇ    Model uses time embeddings + U-Net               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. Calculate Loss                                   ‚îÇ
‚îÇ    loss = MSE(predicted_noise, actual_noise)       ‚îÇ
‚îÇ    loss = mean((predicted - actual)¬≤)               ‚îÇ
‚îÇ    Code: nn.functional.mse_loss()                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 6. Backpropagation & Update                         ‚îÇ
‚îÇ    loss.backward()      # Compute gradients         ‚îÇ
‚îÇ    optimizer.step()     # Update weights            ‚îÇ
‚îÇ    Model gets slightly better at predicting noise   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
        Repeat for next batch!
```

---

### Generation Loop (How New Images Are Created)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Start: Pure Random Noise                            ‚îÇ
‚îÇ    x = random_noise()                               ‚îÇ
‚îÇ    Shape: [64, 1, 28, 28]                           ‚îÇ
‚îÇ    Looks like: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 999 ‚Üí 998                                      ‚îÇ
‚îÇ    predicted_noise = model(x, t=999)                ‚îÇ
‚îÇ    x = remove_noise(x, predicted_noise, t=999)      ‚îÇ
‚îÇ    Still looks like: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 998 ‚Üí 997                                      ‚îÇ
‚îÇ    predicted_noise = model(x, t=998)                ‚îÇ
‚îÇ    x = remove_noise(x, predicted_noise, t=998)      ‚îÇ
‚îÇ    Still very noisy: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
                 ...
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 500 ‚Üí 499 (Halfway)                            ‚îÇ
‚îÇ    predicted_noise = model(x, t=500)                ‚îÇ
‚îÇ    x = remove_noise(x, predicted_noise, t=500)      ‚îÇ
‚îÇ    Vague shapes: ‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñë‚ñë                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
                 ...
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 100 ‚Üí 99                                       ‚îÇ
‚îÇ    predicted_noise = model(x, t=100)                ‚îÇ
‚îÇ    x = remove_noise(x, predicted_noise, t=100)      ‚îÇ
‚îÇ    Clear structure: ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
                 ...
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 1 ‚Üí 0 (Final Step)                             ‚îÇ
‚îÇ    predicted_noise = model(x, t=1)                  ‚îÇ
‚îÇ    x = remove_noise(x, predicted_noise, t=1)        ‚îÇ
‚îÇ    Clean image:  3  (generated digit!)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
              DONE! Return generated images
```

---

### U-Net Forward Pass (What Happens Inside the Model)

```
Input: Noisy image [1, 28, 28] + Timestep t=500
       ‚îÇ
       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ                                     ‚îÇ
       ‚îÇ  Timestep Embedding                 ‚îÇ
       ‚îÇ  t=500 ‚Üí [256] dimensional vector   ‚îÇ
       ‚îÇ  (sinusoidal encoding)              ‚îÇ
       ‚îÇ                                     ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Initial Conv                       ‚îÇ
       ‚îÇ  [1, 28, 28] ‚Üí [64, 28, 28]         ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  DownBlock 1 + Time Embedding       ‚îÇ
       ‚îÇ  [64, 28, 28] ‚Üí [64, 28, 28]        ‚îÇ
       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§‚îÄ‚îÄ‚îê Skip 1
       ‚îÇ  MaxPool                            ‚îÇ  ‚îÇ
       ‚îÇ  [64, 28, 28] ‚Üí [64, 14, 14]        ‚îÇ  ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                         ‚îÇ                       ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
       ‚îÇ  DownBlock 2 + Time Embedding       ‚îÇ  ‚îÇ
       ‚îÇ  [64, 14, 14] ‚Üí [128, 14, 14]       ‚îÇ  ‚îÇ
       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§‚îÄ‚îÄ‚î§ Skip 2
       ‚îÇ  MaxPool                            ‚îÇ  ‚îÇ ‚îÇ
       ‚îÇ  [128, 14, 14] ‚Üí [128, 7, 7]        ‚îÇ  ‚îÇ ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ
                         ‚îÇ                       ‚îÇ ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ
       ‚îÇ  Bottleneck + Time Embedding        ‚îÇ  ‚îÇ ‚îÇ
       ‚îÇ  [128, 7, 7] ‚Üí [128, 7, 7]          ‚îÇ  ‚îÇ ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ
                         ‚îÇ                       ‚îÇ ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ
       ‚îÇ  Transpose Conv (Upsample)          ‚îÇ  ‚îÇ ‚îÇ
       ‚îÇ  [128, 7, 7] ‚Üí [128, 14, 14]        ‚îÇ  ‚îÇ ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ
                         ‚îÇ                       ‚îÇ ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ
       ‚îÇ  Concatenate with Skip 2 ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îò ‚îÇ
       ‚îÇ  [128+128, 14, 14] = [256, 14, 14]  ‚îÇ    ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                         ‚îÇ                         ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
       ‚îÇ  UpBlock 1 + Time Embedding         ‚îÇ    ‚îÇ
       ‚îÇ  [256, 14, 14] ‚Üí [128, 14, 14]      ‚îÇ    ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                         ‚îÇ                         ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
       ‚îÇ  Transpose Conv (Upsample)          ‚îÇ    ‚îÇ
       ‚îÇ  [128, 14, 14] ‚Üí [128, 28, 28]      ‚îÇ    ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                         ‚îÇ                         ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
       ‚îÇ  Concatenate with Skip 1 ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ  [128+64, 28, 28] = [192, 28, 28]   ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  UpBlock 2 + Time Embedding         ‚îÇ
       ‚îÇ  [192, 28, 28] ‚Üí [64, 28, 28]       ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Final Conv                         ‚îÇ
       ‚îÇ  [64, 28, 28] ‚Üí [1, 28, 28]         ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
Output: Predicted noise [1, 28, 28]
```

---

## üí° Why This Approach Works So Well

### 1. **Divide and Conquer**
Instead of one impossible problem ("create a perfect image"), we have 1000 easy problems ("remove a tiny bit of noise").

**Analogy**: 
- Hard: "Build a house in one day"
- Easy: "Lay one brick, repeat 10,000 times"

### 2. **Progressive Refinement**
Each denoising step makes a small improvement. Small improvements are:
- Easier to learn
- More reliable
- Compound to produce excellent results

**Mathematical insight**: The gradient flow is better distributed across timesteps.

### 3. **Single Model, Universal Denoiser**
One U-Net learns to denoise at ALL noise levels (t=0 to t=999).

**How?**: Time embeddings tell the model "this image has 30% noise" vs "this image has 90% noise", so it adapts.

**Benefit**: More efficient than 1000 separate models!

### 4. **Natural Image Prior**
By training on real images, the model learns:
- What digits look like
- What shapes are common
- What textures make sense
- What structures occur together

**Result**: Generated images look realistic because the model learned the "rules" of what makes a valid digit.

### 5. **Noise Prediction is Clever**
Why predict noise instead of the clean image?

**Intuition**: 
- Early steps (t=900): Mostly noise, very little signal ‚Üí Easy to predict noise
- Middle steps (t=500): Mixed ‚Üí Still easier to predict noise than reconstruct full image
- Late steps (t=50): Mostly signal ‚Üí Noise is the small detail to remove

**Result**: Consistent difficulty across all timesteps!

### 6. **Stochastic Generation**
Starting from random noise means:
- Every generation is different (diversity!)
- Can generate infinite variations
- Model doesn't memorize, it creates

**Proof it's not memorizing**: The model was trained on 60,000 images but can generate billions of unique digits.

---

## üìö Further Learning Resources

### üìÑ Foundational Papers (In Reading Order)

**1. The Original DDPM Paper** (Start here!)
- **Title**: "Denoising Diffusion Probabilistic Models" (2020)
- **Authors**: Ho, Jain, Abbeel
- **Why read**: Introduces the core algorithm this project implements
- **Key takeaways**: Noise prediction, variance schedules, training objective
- **Link**: arxiv.org/abs/2006.11239

**2. Improved DDPM** (After understanding basics)
- **Title**: "Improved Denoising Diffusion Probabilistic Models" (2021)
- **Authors**: Nichol & Dhariwal
- **Why read**: Better noise schedules, hybrid objectives
- **Key takeaways**: Cosine schedule, learnable variances
- **Link**: arxiv.org/abs/2102.09672

**3. DDIM** (For faster sampling)
- **Title**: "Denoising Diffusion Implicit Models" (2021)
- **Authors**: Song, Meng, Ermon
- **Why read**: How to sample in 50 steps instead of 1000
- **Key takeaways**: Non-Markovian process, deterministic sampling
- **Link**: arxiv.org/abs/2010.02502

**4. Classifier-Free Guidance** (For better control)
- **Title**: "Classifier-Free Diffusion Guidance" (2022)
- **Authors**: Ho & Salimans
- **Why read**: How to guide generation without a separate classifier
- **Key takeaways**: Conditional vs unconditional training
- **Link**: arxiv.org/abs/2207.12598

---

### üéì Excellent Tutorials & Blog Posts

**1. The Annotated Diffusion Model** (Highly recommended!)
- **Platform**: HuggingFace
- **What**: Line-by-line explanation with code
- **Why**: Complements this project perfectly
- **Link**: huggingface.co/blog/annotated-diffusion

**2. Lilian Weng's Blog**
- **Title**: "What are Diffusion Models?"
- **What**: In-depth mathematical explanation
- **Why**: Best written explanation of the theory
- **Link**: lilianweng.github.io/posts/2021-07-11-diffusion-models/

**3. Assembly AI Tutorial**
- **Title**: "Diffusion Models from Scratch"
- **What**: Video + code walkthrough
- **Why**: Great for visual learners
- **Link**: www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/

**4. Outlier's Blog**
- **Title**: "Diffusion Models: A Practical Guide"
- **What**: Practical implementation details
- **Why**: Bridges theory and practice

---

### üõ†Ô∏è Related Implementations to Study

**1. Stable Diffusion** (Most famous application)
- Applies diffusion in latent space (compressed representations)
- Uses CLIP for text conditioning
- Open-source and accessible
- **GitHub**: github.com/CompVis/stable-diffusion

**2. Improved Diffusion** (OpenAI's implementation)
- Clean PyTorch code
- Includes many improvements
- **GitHub**: github.com/openai/improved-diffusion

**3. Diffusers Library** (HuggingFace)
- Industry-standard library
- Many pre-trained models
- Easy to use
- **GitHub**: github.com/huggingface/diffusers

---

### üì∫ Video Resources

**1. AI Coffee Break** (Conceptual)
- "Diffusion Models Explained"
- Great animations and intuition
- ~15 minutes

**2. Yannic Kilcher** (Technical)
- "DDPM Paper Explained"
- Deep dive into the mathematics
- ~1 hour

**3. Two Minute Papers** (High-level)
- Various diffusion model results
- Shows state-of-the-art capabilities
- Quick overviews

---

## üéØ Mastery Checklist

Use this to track your understanding:

### Conceptual Understanding
- [ ] I can explain diffusion models to a non-technical person
- [ ] I understand why we add noise gradually (forward process)
- [ ] I understand why we remove noise gradually (reverse process)
- [ ] I can explain why we predict noise instead of images
- [ ] I understand the role of time embeddings
- [ ] I can compare diffusion models to GANs and VAEs

### Technical Understanding
- [ ] I understand the forward diffusion formula: `x_t = sqrt(Œ±_t)*x_0 + sqrt(1-Œ±_t)*Œµ`
- [ ] I understand what Œ±_t, Œ≤_t, and variance schedules control
- [ ] I can explain the training objective (MSE on noise prediction)
- [ ] I understand how U-Net architecture works
- [ ] I can explain skip connections and their purpose
- [ ] I understand the role of normalization layers

### Practical Skills
- [ ] I can train a diffusion model from scratch
- [ ] I can generate new images using a trained model
- [ ] I can interpret training loss curves
- [ ] I can debug common issues (OOM, poor quality, etc.)
- [ ] I can modify hyperparameters effectively
- [ ] I can adapt the code to new datasets

### Advanced Topics
- [ ] I understand DDIM and faster sampling methods
- [ ] I know how classifier-free guidance works
- [ ] I can explain latent diffusion (Stable Diffusion)
- [ ] I understand score-based models
- [ ] I can compare different noise schedules
- [ ] I can implement conditional generation

---

## üöÄ Next Steps: Where to Go From Here

### Immediate Next Steps (This Week)
1. ‚úÖ Run all three scripts and observe outputs
2. ‚úÖ Read through this SUMMARY.md completely
3. ‚úÖ Train for 20-50 epochs and compare results
4. ‚úÖ Experiment with different hyperparameters
5. ‚úÖ Generate images and study quality

### Short-term Projects (This Month)
1. **Implement class-conditional generation**
   - Modify model to take digit labels as input
   - Generate specific digits on demand
   - Difficulty: Medium

2. **Try Fashion-MNIST**
   - More challenging than digits
   - Requires same code, different dataset
   - Compare results with MNIST
   - Difficulty: Easy

3. **Implement DDIM sampling**
   - Modify reverse diffusion to skip steps
   - Generate in 50 steps instead of 1000
   - Much faster inference
   - Difficulty: Medium

4. **Visualize intermediate features**
   - Save and plot U-Net activations
   - Understand what the model learns
   - Create attention visualizations
   - Difficulty: Medium

### Medium-term Projects (Next 3 Months)
1. **Scale to CIFAR-10 (32√ó32 color images)**
   - Modify U-Net for larger images and 3 channels
   - Train on more complex dataset
   - Requires more compute (GPU recommended)
   - Difficulty: Hard

2. **Implement better noise schedules**
   - Try cosine schedule
   - Experiment with learned variances
   - Compare results quantitatively
   - Difficulty: Medium

3. **Add conditioning mechanisms**
   - Text conditioning (using CLIP embeddings)
   - Image conditioning (for inpainting)
   - Class conditioning with guidance
   - Difficulty: Hard

4. **Build a web demo**
   - Create a Gradio or Streamlit interface
   - Let users generate images interactively
   - Deploy online
   - Difficulty: Easy-Medium

### Long-term Goals (Next 6-12 Months)
1. **Implement Latent Diffusion**
   - Train an autoencoder (VAE)
   - Apply diffusion in latent space
   - Understand Stable Diffusion architecture
   - Difficulty: Very Hard

2. **Multi-modal conditioning**
   - Text-to-image generation
   - Image editing with text prompts
   - Combine with CLIP or other encoders
   - Difficulty: Very Hard

3. **Research project**
   - Improve sampling speed
   - Better training techniques
   - Novel applications
   - Write a paper!
   - Difficulty: Expert

---

## üåü What Makes This Implementation Special

This project is designed specifically for learning:

### 1. **Minimal but Complete**
- Only ~1000 lines of code total
- No unnecessary abstractions
- Every line serves a purpose
- But implements the full pipeline!

### 2. **Extensively Commented**
- Every function has clear documentation
- Formulas explained in comments
- References to paper sections
- Beginner-friendly variable names

### 3. **Works on Any Hardware**
- Trains on CPU in reasonable time (minutes, not days)
- Automatically uses GPU if available
- Low memory requirements
- No cloud computing needed

### 4. **Actually Works**
- Not just a toy example
- Generates real, high-quality images
- Same algorithm as production systems
- Produces publication-quality results

### 5. **Easy to Extend**
- Modular architecture
- Clear separation of concerns
- Well-structured code
- Many extension points

### 6. **Educational Resources**
- This comprehensive SUMMARY.md
- README with usage examples
- Code comments explaining "why"
- Links to further learning

---

## üéâ Congratulations!

You've completed one of the most comprehensive guides to diffusion models available! 

### What You've Achieved

**You now understand**:
- The fundamental principle behind AI image generation
- How noise addition and removal can create images
- The architecture and training of neural networks
- The mathematics of diffusion processes
- Practical implementation details

**You can now**:
- Build a diffusion model from scratch
- Train models on custom datasets
- Generate new images using trained models
- Debug and optimize training
- Explain diffusion models to others

**You're ready for**:
- Implementing advanced diffusion techniques
- Reading research papers on diffusion
- Contributing to open-source projects
- Building your own creative applications
- Pursuing research in generative AI

---

### The Bigger Picture

This project implements the **same core algorithm** used in:

üé® **Stable Diffusion** (text-to-image)
- Your code: 1000 steps, 28√ó28 pixels, 2M parameters
- Stable Diffusion: 50 steps, 512√ó512 pixels, 890M parameters
- Same principle, scaled up!

üñºÔ∏è **DALL-E 2** (OpenAI's image generator)
- Your code: Digit generation
- DALL-E 2: Photorealistic images from text
- Same denoising process, different conditioning!

üé≠ **Midjourney** (AI art tool)
- Your code: U-Net denoising
- Midjourney: Advanced U-Net + guidance
- Same architecture, refined and optimized!

**The difference is scale, not concept!**

You've learned the foundational algorithm. The production systems add:
- Larger models (billions of parameters)
- More data (millions of images)
- Better conditioning (text, CLIP, etc.)
- Latent space (work on compressed representations)
- Faster sampling (DDIM, DPM-Solver)
- Better guidance (classifier-free)

But the core? **You just built it!** üöÄ

---

### A Final Note

Learning diffusion models is challenging, and you've come a long way! Whether you:
- Followed along step-by-step
- Experimented with the code
- Read through this guide
- Trained your first model

You've accomplished something significant. Diffusion models are at the cutting edge of AI research, and you now have a solid foundation to build upon.

**Keep experimenting. Keep learning. Keep creating.** üåü

---

## üìû Getting Help

**If you're stuck**:
1. Re-read the relevant section in this SUMMARY.md
2. Check the code comments in the source files
3. Review the README.md for usage examples
4. Look at the error message carefully
5. Try the troubleshooting section above
6. Check GitHub issues for similar problems
7. Ask in AI/ML communities (Reddit r/MachineLearning, Discord servers)

**If you want to contribute**:
- This is an educational project, improvements welcome!
- Better documentation is always appreciated
- Bug fixes and optimizations are great
- Share your extensions and experiments

---

**Now go forth and generate! üé®‚ú®**

```bash
python visualize_diffusion.py   # See the process
python train.py --epochs 50     # Train your model
python sample.py --checkpoint outputs/model_final.pt --show_intermediate  # Create art!
```
